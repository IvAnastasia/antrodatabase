# antropodatabase
<b><i>Описание проекта: </i> </b>

<b> Классификация отрывков текстовых расшифровок интервью социолингвистической экспедиции (по темам)</b>

Есть много расшифровок антропологических и социолингвистических интервью из экспедиций Школы лингвистики НИУ ВШЭ в Коми и Карелию, преимущественно на русском языке. В начале каждого документа с расшифровкой (.docx) есть метаинформация (место, дата, информанты и интервьюеры), затем идёт текст интервью, например:

<b>Инф2:</b> Какой язык у тебя родной?

<b>Инт1:</b> Коми и русский.

<b>Инф2:</b> А что роднее?

<b>Инф1:</b> У меня один родитель русский, а второй – коми полностью. 
И с детства поэтому я разговариваю на двух языках, как билингвы.

На основании анализа интервью исследователи пишут статьи, курсовые и т. д. 
Проблема: чтобы, например, написать статью про отношению людей к экологии в республике Коми, надо в каждой расшифровке интервью вручную искать слова «экология», «нефтеразлив», «Лукойл». Это очень неудобно.
Выход: присвоить каждой реплике интервью теги тем, использовав для этого машинное обучение + придумать, как удобно хранить много таких расшифровок

--Новое решение:--
Сделаем <b>корпус</b> на основе tsakorpus: https://tsakorpus.readthedocs.io/en/latest/
corpora_making код, превращающий текстовые расшифровки и информацию про интервью из таблицы в джейосоны, которые tsakorpus индексирует с помощью elasticsearch. Далее мы создали докер-контейнер и выпустили сайт
Сейчас я добавляю поиск по леммам и по темам, уже доступен поиск точных соответствий (необязатально целых слов, надо нажать "поиск предложений") и поиск по метаинформации. 

В собранных нами интервью есть конфиденциальная информация, поэтому, к сожалению, мы не можем выложить в открытый доступ полную версию корпуса. Демонстрационная версия с 30 интервью из Карелии и без половины метаданных: http://northern-archive.ru:7342/search

Постер сентябрьской верссии проекта: https://drive.google.com/file/d/1uhNr2Wvfb0D31jyWQvWhuhB6L9KN6z7x/view

Проект поддержан НИУ ВШЭ.


---Старая версия: ----
<i> Этапы работы: </i> 
1) код, который достатнет все расшифровки из папки + парсер метаинформации интервью и высказываний
2) код, записывающий всю найденную информацию в базу данных .db в четыре таблицах с данными про интервью, интервьюеров+информантов, высказывания (=реплики), реальные имена информантов и интервьюеров (отдельно для безопасности персональных данных).
3) чтение базы данных в питоне, работа с датафреймом:
4) ручная разметка 200 реплик по 10 темам, то есть подобор тега, который анилучшим образом описывает тему отрывка.
5) skitlearn модель, которая на принимает на вход размеченные мной высказывания и сделает классификацию остальных высказываний + проверка accuracy score модели.
4) анализ данных + визуализуализация

В перспективе (помимо доработки недочётов):
сделать сайт с удобным интерфейсом, на который будет выложена база данных для внутреннего пользования членов экспедиций.

<i> Подробнее -- в презентации.</i> 
Ссылка на презентацию:

https://docs.google.com/presentation/d/1x8OBthXDp3DLF4yPsWL7wfaW3xE7qvCPu8rgUbvElDk/edit?usp=sharing

Я не выкладываю базу, потому что в ней много личных данных информантов и интервьюеров (выкладываю скрины).
